{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Y-NbExJ595"
      },
      "source": [
        "# E-tivity 2 Weeks 3-4\n",
        "\n",
        "* Barry Clarke\n",
        "\n",
        "* 24325082"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu_3M0LLJ5-B"
      },
      "source": [
        "## Outline\n",
        "\n",
        "\n",
        "The second <a href=\"#part2\">**Sentiment Analysis**</a> is an individual task, the problem is to predict if a film review is positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96_ZTrIJ5-E"
      },
      "source": [
        "### Guidelines\n",
        "\n",
        "The e-tivity is split into five tasks. The first four are \"group\" excersises, in that you post the solutions to Tasks 1-4 to the Group Locker. This will allow the members of your group to send you feedback (via the forums) so you can improve your submission. The final task is an individual task and together with the other tasks, should be uploaded to Brightspace.\n",
        "\n",
        "Marks will be deducted if task 5 is uploaded in contravention of instructions. Also if the the final submission is not a single notebook with tasks 1-5 and with correct identification or filename.\n",
        "\n",
        "\n",
        "Grading guidelines: the rubrics for the e-tivity are here https://learn.ul.ie/d2l/lp/rubrics/preview.d2l?ou=73310&rubricId=4446&originTool=quicklinks\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEx6cp55J5-N"
      },
      "source": [
        "<a id='part2'></a>\n",
        "## Sentiment Analysis\n",
        "\n",
        "\n",
        "This task is a individual task and should **not** to be uploaded to the Group Locker. No posts should be given via the forums. Marks will be deducted if the instructions are not followed (see rubrics). This part should be uploaded directly to Brightpsace.\n",
        "\n",
        "### Context\n",
        "\n",
        "You have a model that predicts the sentiment of a film review (positive or negative) from the IMDB. There are two hyperparameters that format the data from IMDB: the maximum review length and the dictionary size. Below is a RNN model that predicts sentiment values.\n",
        "\n",
        "\n",
        "### Task 5:  (completed by Sunday Week 4)\n",
        "\n",
        "Keeping top_words, max_review_lenngth and the embedding_vector_length the same, change the model so that it uses attention only, i.e. there are no recurrent components. The only Keras layers (defined here https://www.tensorflow.org/api_docs/python/tf/keras/layers) that you allowed to use are:\n",
        "- Input,\n",
        "- Embedding,\n",
        "- Dense,\n",
        "- Any Attention (must be at leat one),\n",
        "- TimeDistributed,\n",
        "- Any Merging,\n",
        "- Any Reshaping,\n",
        "- Any Pooling,\n",
        "- Dropout.\n",
        "\n",
        "You need not use any of the layers (except attention) but you can use some more than once. Can you do at least as good as the RNN example?\n",
        "\n",
        "**NB** There are many examples of using attention for sentiment analysis but we looking to see if you can construct a sensible model. The model will be delibrately restricted, so do not waste too much time achieving spectacular accuracy. Remember the rules of thumb that we discussed in the Introduction to DL module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7NOQ5jIJ5-O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "top_words = 100\n",
        "(Rev_train, Sc_train), (Rev_test, Sc_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(Rev_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(Rev_test, maxlen=max_review_length)\n",
        "print(X_train.shape)\n",
        "\n",
        "embedding_vector_length = 16\n",
        "model_imdb = Sequential()\n",
        "model_imdb.add(Input(shape=(max_review_length,)))\n",
        "model_imdb.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(LSTM(10))\n",
        "model_imdb.add(Dropout(0.2))\n",
        "model_imdb.add(Dense(1, activation='sigmoid'))\n",
        "model_imdb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model_imdb.summary())\n",
        "#plot_model(model_imdb,show_shapes=True)\n",
        "\n",
        "nepoch = 150\n",
        "#IMDB_history = model_imdb.fit(X_train,Sc_train,validation_data=(X_test,Sc_test),epochs=nepoch,batch_size=256)\n",
        "IMDB_history=np.load('IMDBTrainhist.npy',allow_pickle='TRUE').item()\n",
        "\n",
        "plt.plot(range(nepoch),IMDB_history.history['loss'],c='r')\n",
        "plt.plot(range(nepoch),IMDB_history.history['val_loss'],c='b')\n",
        "plt.plot()\n",
        "\n",
        "plt.plot(range(nepoch),IMDB_history.history['accuracy'],c='r')\n",
        "plt.plot(range(nepoch),IMDB_history.history['val_accuracy'],c='b')\n",
        "plt.plot()\n",
        "\n",
        "# --- Final Quantitative Result ---\n",
        "# Pull the last value from the validation history to show the 'Test' result\n",
        "final_rnn_loss = IMDB_history.history['val_loss'][-1]\n",
        "final_rnn_acc = IMDB_history.history['val_accuracy'][-1]\n",
        "\n",
        "print(\"\\n--- Final Evaluation (Baseline) ---\")\n",
        "print(f\"RNN Baseline Test Loss: {final_rnn_loss:.4f}\")\n",
        "print(f\"RNN Baseline Test Accuracy: {final_rnn_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPmCmM58J5-R"
      },
      "source": [
        "\n",
        "The history for the model above has been saved, as it takes a while to run. If you want to run it yourself then comment out the second line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afxUXC7aJ5-S"
      },
      "source": [
        "### Final Submission (complete by Sunday Week 4)\n",
        "\n",
        "Submit Tasks 1-5 in a single notebook this before the deadline on Sunday.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeu95USVJ5-T"
      },
      "outputs": [],
      "source": [
        "## Add additional code cells to implememt the tasks stated above"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Attention, GlobalAveragePooling1D, AveragePooling1D\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Maintain the required hyperparameters\n",
        "top_words = 100\n",
        "max_review_length = 500\n",
        "embedding_vector_length = 16\n",
        "nepoch_attention = 10 # Set to 10 for a good balance of speed and visual results - changed from 10 (70.85%) to 20 (72.36%) to 30 (72.69%)\n",
        "\n",
        "# final model 72.54% #3\n",
        "# Construct the Attention-Only Model (respecting task instructions and only using permitted layers)\n",
        "inputs = Input(shape=(max_review_length,))\n",
        "x = Embedding(input_dim=top_words, output_dim=embedding_vector_length)(inputs)\n",
        "\n",
        "# Use a Dense layer to project the embeddings - this often helps Attention find better patterns\n",
        "x_projected = Dense(embedding_vector_length, activation='relu')(x)\n",
        "\n",
        "# SPEED FIX: Increase pool_size to 10: this reduces the number of 'query points' from 100 to 50, making it much faster\n",
        "query = AveragePooling1D(pool_size=10)(x_projected)\n",
        "\n",
        "# Use Attention: comparing the 50 'summary chunks' against all 500 words\n",
        "att_out = Attention()([query, x_projected])\n",
        "\n",
        "# Pool the attention results\n",
        "x = GlobalAveragePooling1D()(att_out)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# 3. Initialise and Compile\n",
        "attention_model = Model(inputs=inputs, outputs=outputs)\n",
        "attention_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# --- Output 1: Model Summary ---\n",
        "print(\"--- Attention Model Architecture ---\")\n",
        "attention_model.summary()\n",
        "\n",
        "# --- Output 2: Training Phase ---\n",
        "print(\"\\n--- Training Phase ---\")\n",
        "history_attention = attention_model.fit(\n",
        "    X_train, Sc_train,\n",
        "    validation_data=(X_test, Sc_test),\n",
        "    epochs=nepoch_attention,\n",
        "    batch_size=256\n",
        ")\n",
        "\n",
        "# --- Output 3: Visualisation ---\n",
        "# Create a figure with two subplots\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Loss Curves\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(nepoch_attention), history_attention.history['loss'], c='r', label='Train Loss')\n",
        "plt.plot(range(nepoch_attention), history_attention.history['val_loss'], c='b', label='Val Loss')\n",
        "plt.title('Attention Model: Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot 2: Accuracy Curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(nepoch_attention), history_attention.history['accuracy'], c='r', label='Train Acc')\n",
        "plt.plot(range(nepoch_attention), history_attention.history['val_accuracy'], c='b', label='Val Acc')\n",
        "plt.title('Attention Model: Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Output 4: Final Quantitative Result ---\n",
        "test_loss, test_acc = attention_model.evaluate(X_test, Sc_test, verbose=0)\n",
        "print(f\"\\nFinal Attention Model Test Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "t3alrMHO_po7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9oqBOn7J5-U"
      },
      "source": [
        "## Reflection\n",
        "\n",
        "## Task 5: Sentiment Analysis with Attention-Only Architecture\n",
        "\n",
        "### Model Evolution and Iterative Design\n",
        "\n",
        "The objective of this task was to construct a \"sensible\" sentiment analysis model using only attention mechanisms and permitted Keras layers, strictly avoiding recurrent components such as LSTMs. Three different architectural approaches were developed to explore the relationship between attention queries and classification accuracy.\n",
        "\n",
        "#### 1. Baseline Attention Model (Iteration #1)\n",
        "\n",
        "The first model used a **GlobalAveragePooling1D** layer to compress the entire review into a single summary vector. This summary was then used as the **Query** in the attention mechanism to identify important words in the original sequence. Although this was a clear and logical starting point, compressing the review into one vector reduced the amount of detailed information available. As a result, the model achieved a validation accuracy of approximately **70.85%**.\n",
        "\n",
        "#### 2. Downsampled Chunk Analysis (Iteration #2)\n",
        "\n",
        "The second model replaced the single global summary with an **AveragePooling1D** layer (pool size 5). Instead of compressing the whole review into one vector, the sequence was divided into smaller chunks. This allowed the attention layer to work with more detailed local information. As a result, validation accuracy improved to **72.36%**, showing that preserving local context helps sentiment classification.\n",
        "\n",
        "#### 3. Projected Embedding & Speed Optimisation (Iteration #3)\n",
        "\n",
        "The final model added a **Dense** layer with **ReLU** activation to transform the embeddings before applying attention. The pooling size was also increased from 10 to 20 and then 30 to attempt to improve computational efficiency. This aimed to help the model capture slightly more complex patterns. After training for 30 epochs, performance reached **72.54%**, showing only a small improvement.\n",
        "\n",
        "#### 4. High-Capacity Attention Variant (Exploratory – No Improvement)\n",
        "\n",
        "An additional high-capacity variant (included as commented-out code) expanded the embedding representation using a larger **Dense (32)** projection layer before attention and added another **Dense (16)** layer after pooling. This increased the model’s representational capacity and complexity while still respecting all layer constraints.\n",
        "\n",
        "However, this modification produced only a marginal increase in accuracy (approximately **72.78%**), which was not meaningfully better than the simpler model. This suggests that the limitation is not model capacity, but rather the restricted vocabulary size and absence of sequential modelling. As a result, the simpler architecture was retained.\n",
        "\n",
        "---\n",
        "\n",
        "### Graphical Analysis and Diagnostics\n",
        "\n",
        "The training and validation graphs show that the learning process is stable and well optimised:\n",
        "\n",
        "- **Consistent Convergence:** The loss graph shows both training and validation curves decreasing sharply before flattening out, indicating that the model reached stability without divergence.\n",
        "\n",
        "- **Absence of Overfitting:** The accuracy graph shows the validation curve (blue) closely following the training curve (red). The lack of a widening gap indicates that the model generalises well and that the Dropout layers helped prevent overfitting.\n",
        "\n",
        "- **Early Signal Capture:** Most performance improvements occur within the first 4 epochs, with accuracy plateauing around 72.5% shortly afterwards.\n",
        "\n",
        "- **Vocabulary Ceiling:** The rapid plateau indicates a performance ceiling, suggesting that the model quickly exhausted the predictive signal available within the 100-word dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion and Performance Analysis\n",
        "\n",
        "The results suggest that attention-only models have a performance limit when restricted to a vocabulary of 100 words. Although the models were logically designed and stable, they performed about 5% below the RNN baseline (77.23%).\n",
        "\n",
        "This difference is most likely due to the limited vocabulary size. With only 100 words available, the order of words (captured by the RNN) becomes more important than simply weighting important words (captured by Attention). While the attention-only model cannot fully match the RNN under these constraints, it still provides a simple and parallelisable alternative for sentiment classification.\n",
        "\n",
        "An additional experiment was conducted by increasing the vocabulary size (**top_words**) from 100 to 200. This yielded no significant improvement in accuracy. This further supports the conclusion that performance is constrained primarily by architectural restrictions and the absence of sequential processing rather than insufficient model capacity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Baseline model #1\n",
        "# 2. Construct the Attention-Only Model (Strictly using permitted layers)\n",
        "inputs = Input(shape=(max_review_length,))\n",
        "x = Embedding(input_dim=top_words, output_dim=embedding_vector_length)(inputs)\n",
        "\n",
        "# Create a 'Query' by pooling the whole sequence into a single summary vector\n",
        "# This represents the 'general gist' of the review\n",
        "query = GlobalAveragePooling1D()(x)\n",
        "# Reshape so it can be compared back to the original sequence\n",
        "query = Reshape((1, embedding_vector_length))(query)\n",
        "\n",
        "# Use Attention to let the 'summary' pick out important words from the sequence (x)\n",
        "att_out = Attention()([query, x])\n",
        "\n",
        "# Flatten the attention result and predict\n",
        "#x = GlobalAveragePooling1D()(att_out)\n",
        "x = Reshape((embedding_vector_length,))(att_out)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "'''\n",
        "\n",
        "'''\n",
        "# Next change to model #2\n",
        "# 2. Construct the Attention-Only Model (Strictly using permitted layers)\n",
        "inputs = Input(shape=(max_review_length,))\n",
        "x = Embedding(input_dim=top_words, output_dim=embedding_vector_length)(inputs)\n",
        "\n",
        "# Create a richer query by downsampling the sequence slightly\n",
        "# This helps the model look at 'chunks' of text rather than just one global average\n",
        "query = AveragePooling1D(pool_size=5)(x)\n",
        "\n",
        "# Use Attention: the model compares the 'local chunks' (query) to all words (x)\n",
        "att_out = Attention()([query, x])\n",
        "\n",
        "# Pool the attention results to get a final sentiment feature\n",
        "x = GlobalAveragePooling1D()(att_out)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "'''\n",
        "'''\n",
        "# --- High-Capacity Attention Model --- only 72.78% accuracy - reverting\n",
        "inputs = Input(shape=(max_review_length,))\n",
        "x = Embedding(input_dim=top_words, output_dim=embedding_vector_length)(inputs)\n",
        "\n",
        "# NEW: Add a Dense layer immediately after Embedding to 'expand' the word features\n",
        "# This helps the model find more complex relationships in the 100-word set\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "# Your 'Sensible' Query logic\n",
        "query = AveragePooling1D(pool_size=5)(x)\n",
        "att_out = Attention()([query, x])\n",
        "\n",
        "# Final processing\n",
        "x = GlobalAveragePooling1D()(att_out)\n",
        "\n",
        "# NEW: Another Dense layer to 'think' about the attention results before predicting\n",
        "x = Dense(16, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "'''"
      ],
      "metadata": {
        "id": "tQx7iAAminWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}