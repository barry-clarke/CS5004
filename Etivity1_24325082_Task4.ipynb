{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okirBbgk4pvs"
      },
      "source": [
        "# E-tivity 1 (Weeks 1-2)\n",
        "\n",
        "* Barry Clarke\n",
        "\n",
        "* 24325082"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlaixIws4pvt"
      },
      "source": [
        "## Anomaly Detection\n",
        "\n",
        "### Context\n",
        "We have a mystery dataset. There are 9 explanatory variables and one response variable. The response variable is the last column and indicates if the sample is anomalous (=1, valid =0). The dataset is provided \"data.csv\".\n",
        "\n",
        "Of course in this case we could use supervised learning to generate a model and detect anomalies in new data. However the focus is on autoencoders, anomaly detection is just one of the potential uses for autoencoders.\n",
        "\n",
        "So we are going to pretend that we do not know which data are anomalous but we do know that the anomaly rate is small. Use an autoencoder to detect anomalies in the data. The correctness of the model can of course be checked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zeAa6yI4pvv"
      },
      "source": [
        "### Task 4: VAE (completed by Sunday Week 2)\n",
        "\n",
        "This task is a individual task and should **not** to be uploaded to the Group Locker. No direct support should be given via the forums. Marks will be deducted if the instructions are not followed (see rubrics). This part should be uploaded directly to Brightpsace.\n",
        "\n",
        "Change the network to be a VAE. Again determine the optimal cutoff and plot the latent variables. Check how good the cutoffs were by constructing a confusion matrix or generating a classification report. Obviously for this task you need to use the Anom column.\n",
        "\n",
        "**Hint** you can use the model topology from the AE (with the obvious modifications). I found that I had a good model (almost as good and the supervised learning model) when the KL divergence was small. You can print out both the KL divergence and reconstruction loss for each epoch. It can be tricky to train these type of models, so do not be surprised if you do not get a stellar result. What is more important is that you have the correct code to implement the VAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UekzPZL94pvw"
      },
      "source": [
        "### Final Submission (complete by Sunday Week 2)\n",
        "\n",
        "Submit Tasks 1-4 in a single notebook this before the deadline on Sunday.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Task 4: Variational Autoencoder (VAE)\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import time  # <--- Added for timer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Data Preprocessing\n",
        "# ---------------------------------------------------------\n",
        "print(\"--- Data Preprocessing ---\")\n",
        "# Load Data\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Anom', axis=1)\n",
        "y = df['Anom']\n",
        "\n",
        "# Split Data (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training shape: {X_train_scaled.shape}\")\n",
        "print(f\"Test shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. VAE Model Definition\n",
        "# ---------------------------------------------------------\n",
        "class DenseVAE(tf.keras.Model):\n",
        "    def __init__(self, input_dim, latent_dim=2):\n",
        "        super(DenseVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # --- Encoder ---\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(shape=(input_dim,)),\n",
        "            Dense(7, activation='relu'),\n",
        "            Dense(4, activation='relu'),\n",
        "            Dense(latent_dim + latent_dim) # Output: mean and logvar\n",
        "        ])\n",
        "\n",
        "        # --- Decoder ---\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
        "            Dense(4, activation='relu'),\n",
        "            Dense(7, activation='relu'),\n",
        "            Dense(input_dim, activation='linear')\n",
        "        ])\n",
        "\n",
        "        # Trackers for metrics\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        eps = tf.random.normal(shape=tf.shape(mean))\n",
        "        return eps * tf.exp(logvar * 0.5) + mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, logvar = self.encode(inputs)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # 1. Forward pass\n",
        "            mean, logvar = self.encode(data)\n",
        "            z = self.reparameterize(mean, logvar)\n",
        "            reconstruction = self.decode(z)\n",
        "\n",
        "            # 2. Calculate Losses\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(tf.square(data - reconstruction), axis=1)\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # 3. Backpropagation\n",
        "        grads = tape.gradient(total_loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        # 4. Update metrics\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Model Training\n",
        "# ---------------------------------------------------------\n",
        "# Define a callback to print progress every 10 epochs\n",
        "class PrintProgress(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}: Loss={logs['loss']:.4f}, Recon={logs['reconstruction_loss']:.4f}, KL={logs['kl_loss']:.4f}\")\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "vae = DenseVAE(input_dim, latent_dim=2)\n",
        "vae.compile(optimizer=Adam(learning_rate=0.0005))\n",
        "\n",
        "print(\"\\n--- Starting VAE Training ---\")\n",
        "\n",
        "# Start Timer\n",
        "start_time = time.time()\n",
        "\n",
        "history = vae.fit(\n",
        "    X_train_scaled,\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    verbose=0,\n",
        "    callbacks=[PrintProgress()]\n",
        ")\n",
        "\n",
        "# End Timer\n",
        "end_time = time.time()\n",
        "total_seconds = end_time - start_time\n",
        "minutes, seconds = divmod(total_seconds, 60)\n",
        "\n",
        "print(\"Training complete.\")\n",
        "print(f\"Total training time: {int(minutes)} min {seconds:.2f} sec\")\n",
        "\n",
        "# Plot Loss Curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['reconstruction_loss'], label='Reconstruction Loss')\n",
        "plt.plot(history.history['kl_loss'], label='KL Loss')\n",
        "plt.plot(history.history['loss'], label='Total Loss', linestyle='--')\n",
        "plt.title('VAE Training Loss Components')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Anomaly Detection\n",
        "# ---------------------------------------------------------\n",
        "# Predict on test set\n",
        "reconstructions = vae.predict(X_test_scaled)\n",
        "mse = np.mean(np.power(X_test_scaled - reconstructions, 2), axis=1)\n",
        "\n",
        "# Determine Cutoff (92nd percentile)\n",
        "threshold = np.percentile(mse, 92)\n",
        "print(f\"\\nCalculated Threshold (92nd percentile): {threshold:.4f}\")\n",
        "\n",
        "# Histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "counts, bins, patches = plt.hist(mse, bins=50, alpha=0.75, label='Reconstruction Error')\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
        "plt.text(threshold, max(counts)*0.8, f' Cutoff\\n {threshold:.4f}', color='red')\n",
        "plt.title('VAE Reconstruction Error Histogram')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Evaluation\n",
        "# ---------------------------------------------------------\n",
        "y_pred = [1 if e > threshold else 0 for e in mse]\n",
        "\n",
        "print(\"\\nVAE Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix (Task 4: VAE)')\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. Latent Space Visualization\n",
        "# ---------------------------------------------------------\n",
        "z_mean, z_log_var = vae.encode(X_test_scaled)\n",
        "z_mean = z_mean.numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test, cmap='coolwarm', alpha=0.6, s=10)\n",
        "plt.colorbar(scatter, label='Anomaly (0=Normal, 1=Anom)')\n",
        "plt.title('Latent Space Representation (2D Mean)')\n",
        "plt.xlabel('Latent Dim 1')\n",
        "plt.ylabel('Latent Dim 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYXBjR7a8fqt",
        "outputId": "ca8dad95-a25c-4ec1-a215-5002c6c63eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Data Preprocessing ---\n",
            "Training shape: (39277, 9)\n",
            "Test shape: (9820, 9)\n",
            "\n",
            "--- Starting VAE Training ---\n",
            "Epoch 10: Loss=nan, Recon=nan, KL=nan\n",
            "Epoch 20: Loss=nan, Recon=nan, KL=nan\n",
            "Epoch 30: Loss=nan, Recon=nan, KL=nan\n",
            "Epoch 40: Loss=nan, Recon=nan, KL=nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9q8IVls4pvw"
      },
      "source": [
        "## Reflection\n",
        "\n",
        "There are no specific marks allocated for a reflection. However due consideration will be given if pertinent comments or valuable insights are made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbcfr5vX4pvw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}