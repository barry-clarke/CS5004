{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okirBbgk4pvs"
      },
      "source": [
        "# E-tivity 1 (Weeks 1-2)\n",
        "\n",
        "* Barry Clarke\n",
        "\n",
        "* 24325082"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlaixIws4pvt"
      },
      "source": [
        "## Anomaly Detection\n",
        "\n",
        "### Context\n",
        "We have a mystery dataset. There are 9 explanatory variables and one response variable. The response variable is the last column and indicates if the sample is anomalous (=1, valid =0). The dataset is provided \"data.csv\".\n",
        "\n",
        "Of course in this case we could use supervised learning to generate a model and detect anomalies in new data. However the focus is on autoencoders, anomaly detection is just one of the potential uses for autoencoders.\n",
        "\n",
        "So we are going to pretend that we do not know which data are anomalous but we do know that the anomaly rate is small. Use an autoencoder to detect anomalies in the data. The correctness of the model can of course be checked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zeAa6yI4pvv"
      },
      "source": [
        "### Task 4: VAE (completed by Sunday Week 2)\n",
        "\n",
        "This task is a individual task and should **not** to be uploaded to the Group Locker. No direct support should be given via the forums. Marks will be deducted if the instructions are not followed (see rubrics). This part should be uploaded directly to Brightpsace.\n",
        "\n",
        "Change the network to be a VAE. Again determine the optimal cutoff and plot the latent variables. Check how good the cutoffs were by constructing a confusion matrix or generating a classification report. Obviously for this task you need to use the Anom column.\n",
        "\n",
        "**Hint** you can use the model topology from the AE (with the obvious modifications). I found that I had a good model (almost as good and the supervised learning model) when the KL divergence was small. You can print out both the KL divergence and reconstruction loss for each epoch. It can be tricky to train these type of models, so do not be surprised if you do not get a stellar result. What is more important is that you have the correct code to implement the VAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UekzPZL94pvw"
      },
      "source": [
        "### Final Submission (complete by Sunday Week 2)\n",
        "\n",
        "Submit Tasks 1-4 in a single notebook this before the deadline on Sunday.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time"
      ],
      "metadata": {
        "id": "HQNW-i8w9X9O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Data Preprocessing (Matching Task 1-3)\n",
        "\n",
        "# Load Data\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Separate features and target\n",
        "# The target variable 'Anom' is isolated from the input features (X).\n",
        "# Although 'Anom' is not used for training the autoencoder, it is required\n",
        "# for stratified splitting and the final evaluation\n",
        "X = df.drop('Anom', axis=1)\n",
        "y = df['Anom']\n",
        "\n",
        "# Create 3-Way Split (Train / Val / Test) - as per Task1-3\n",
        "# A 3-way split is implemented to ensure the model is trained, tuned, and tested on distinct subsets\n",
        "# This partition is isolated immediately and is never seen by the model during the training phase\n",
        "# 'stratify=y' ensures the proportion of anomalies is consistent with the original dataset\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Validation set (20% of remaining = 16% total) - For Early Stopping\n",
        "# The remaining 80% of the data is split again\n",
        "# This validation set is used to monitor performance during training (e.g., for Early Stopping),\n",
        "# preventing the model from overfitting to the training data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Scale Data (StandardScaler to match AE logic)\n",
        "# Standardisation is applied to normalise the feature range\n",
        "# Important: The scaler is fitted ONLY on the Training data to avoid data leakage\n",
        "# The mean and variance learned from X_train are then applied to transform X_val and X_test\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training Shape:   {X_train_scaled.shape}\")\n",
        "print(f\"Validation Shape: {X_val_scaled.shape}\")\n",
        "print(f\"Test Shape:       {X_test_scaled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tRsB9R7BE-74",
        "outputId": "ac51e41f-4c4b-4698-dce2-384df00cb56f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2396448645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: VAE Model Definition\n",
        "# REF: https://keras.io/examples/generative/vae/\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# REF: https://keras.io/examples/generative/vae/\n",
        "# A custom model class is utilised to handle the compound loss function\n",
        "# (Reconstruction + KL Divergence) computed internally\n",
        "class VAE(Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.recon_loss_tracker = tf.keras.metrics.Mean(name=\"recon_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.recon_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction\n",
        "\n",
        "    # REF: https://keras.io/examples/generative/vae/\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            # --- Reconstruction Loss (MSE) ---\n",
        "            # Measures fidelity by calculating the error between input and output\n",
        "            squared_errors = tf.square(data - reconstruction)\n",
        "            sum_squared_errors = tf.reduce_sum(squared_errors, axis=1)\n",
        "            recon_loss = tf.reduce_mean(sum_squared_errors)\n",
        "\n",
        "            # --- KL Divergence ---\n",
        "            # Acts as a regulariser, forcing the latent distribution to approximate a standard normal Gaussian\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "\n",
        "            # Total Loss\n",
        "            total_loss = recon_loss + kl_loss\n",
        "\n",
        "        # Backpropagation and weight updates\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.recon_loss_tracker.update_state(recon_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"recon_loss\": self.recon_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Consistent calculation for Validation/Test\n",
        "        squared_errors = tf.square(data - reconstruction)\n",
        "        recon_loss = tf.reduce_mean(tf.reduce_sum(squared_errors, axis=1))\n",
        "\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        total_loss = recon_loss + kl_loss\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.recon_loss_tracker.update_state(recon_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"recon_loss\": self.recon_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "def build_vae_architecture(input_dim):\n",
        "    # --- Encoder ---\n",
        "    # The encoder compresses the input data into a lower-dimensional latent space\n",
        "    encoder_inputs = layers.Input(shape=(input_dim,))\n",
        "    # The topology mirrors the standard Autoencoder (Tasks 1-3) for fair comparison\n",
        "    x = layers.Dense(14, activation=\"relu\")(encoder_inputs) # Matching AE capacity from tasks1-3\n",
        "    x = layers.Dense(6, activation=\"relu\")(x)\n",
        "\n",
        "    # Latent space (Mean and Log Variance)\n",
        "    # Unlike a standard Autoencoder which maps inputs to a single point,\n",
        "    # The VAE maps inputs to a probability distribution defined by a mean and variance\n",
        "    z_mean = layers.Dense(2, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(2, name=\"z_log_var\")(x)\n",
        "\n",
        "    # The sampling layer generates a random point 'z' from the learned distribution\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    # The encoder model is instantiated to output both the distribution parameters\n",
        "    # (for calculating KL loss) and the sampled point (for the decoder)\n",
        "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    # --- Decoder ---\n",
        "    # The decoder attempts to reconstruct the original input from the sampled point 'z'\n",
        "    latent_inputs = layers.Input(shape=(2,))\n",
        "\n",
        "    # The architecture mirrors the encoder to restore the original input dimensions\n",
        "    x = layers.Dense(6, activation=\"relu\")(latent_inputs)\n",
        "    x = layers.Dense(14, activation=\"relu\")(x)\n",
        "\n",
        "    # Linear activation is selected as the targets are continuous, standardised values\n",
        "    decoder_outputs = layers.Dense(input_dim, activation=\"linear\")(x)\n",
        "\n",
        "    decoder = Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    # --- VAE ---\n",
        "    # The full Variational Autoencoder is assembled by connecting the encoder and decoder\n",
        "    vae = VAE(encoder, decoder)\n",
        "\n",
        "    # The model is compiled with the Adam optimiser\n",
        "    # A learning rate of 0.001 is selected to ensure stable convergence\n",
        "    vae.compile(optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "    return vae, encoder, decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "pKNVauQn1hyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Training (Best of 5)\n",
        "\n",
        "# Define the Helper to print every 10 epochs\n",
        "# A custom callback is used to reduce console clutter, printing updates only every 10th epoch\n",
        "class PrintProgress(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"  Epoch {epoch + 1}: loss={logs['loss']:.4f}, val_loss={logs['val_loss']:.4f}, \"\n",
        "                  f\"val_recon={logs['val_recon_loss']:.4f}, val_kl={logs['val_kl_loss']:.4f}\")\n",
        "\n",
        "# Variables are initialised to track the best performing model across the 5 runs\n",
        "best_val_loss = float('inf')\n",
        "best_vae = None\n",
        "best_encoder = None\n",
        "best_history = None\n",
        "\n",
        "print(\"Starting VAE Training (Best of 5)...\")\n",
        "\n",
        "# The training process is repeated 5 times to mitigate the impact of random weight initialisation\n",
        "for i in range(5):\n",
        "    print(f\"\\n--- Run {i + 1} of 5 ---\")\n",
        "\n",
        "    # A fresh VAE model is built for each iteration\n",
        "    vae, encoder, decoder = build_vae_architecture(X_train_scaled.shape[1])\n",
        "\n",
        "    # Early Stopping halts training if validation loss stops improving for 5 epochs\n",
        "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Explicitly using X_val for validation (Corrects Leakage)\n",
        "    # The model trains on the Training set and validates against the dedicated Validation set\n",
        "    # Verbose is set to 0 to use the custom 'PrintProgress' callback instead\n",
        "    history_run = vae.fit(\n",
        "        X_train_scaled,\n",
        "        epochs=100,\n",
        "        batch_size=64,\n",
        "        validation_data=(X_val_scaled, X_val_scaled),\n",
        "        callbacks=[es, PrintProgress()],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Check result (Total Val Loss)\n",
        "    # The lowest validation loss achieved during this specific run is retrieved\n",
        "    val_loss = min(history_run.history['val_loss'])\n",
        "    final_recon = history_run.history['val_recon_loss'][-1]\n",
        "    final_kl = history_run.history['val_kl_loss'][-1]\n",
        "\n",
        "    print(f\"Run {i+1} Finished. Val Loss: {val_loss:.4f} (Recon: {final_recon:.4f}, KL: {final_kl:.4f})\")\n",
        "\n",
        "    # If the current run is better than the previous best, the model and history are saved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_vae = vae\n",
        "        best_encoder = encoder\n",
        "        best_history = history_run\n",
        "\n",
        "print(f\"\\nBest Run Validation Loss: {best_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "_jpmu6p3FRID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# Task 4: Results & Visualisation\n",
        "# ==========================================\n",
        "\n",
        "# Plot Training History (Losses)\n",
        "# The training dynamics are visualised\n",
        "# Both the total loss and its components (Reconstruction and KL Divergence) are plotted\n",
        "# to monitor convergence and check for issues like posterior collapse\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(best_history.history['loss'], label='Total Train Loss')\n",
        "plt.plot(best_history.history['val_loss'], label='Total Val Loss')\n",
        "plt.plot(best_history.history['recon_loss'], label='Recon Train Loss', linestyle='--')\n",
        "plt.plot(best_history.history['kl_loss'], label='KL Train Loss', linestyle=':')\n",
        "plt.title('VAE Training Dynamics (Best Run)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Latent Space Visualisation\n",
        "# The test data is compressed into the 2D latent space\n",
        "# Only 'z_mean' is used for visualisation, as it represents the centre of the learned distribution for each point\n",
        "z_mean, _, _ = best_encoder.predict(X_test_scaled)\n",
        "\n",
        "# A scatter plot visualises how the VAE clusters normal data versus anomalies\n",
        "# This helps confirm if the model has learned to separate the classes spatially\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test, cmap='coolwarm', alpha=0.6, s=10)\n",
        "plt.colorbar(scatter, label='Anomaly (0=Normal, 1=Anom)')\n",
        "plt.title('Latent Space Representation (2D)')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.show()\n",
        "\n",
        "# --- MODIFIED SECTION START ---\n",
        "# Anomaly Detection (Reconstruction Error)\n",
        "# Establish Threshold using Training Data ONLY\n",
        "# Consistent with Task 3 feedback: learn the threshold from the Training set\n",
        "train_reconstructions = best_vae.predict(X_train_scaled)\n",
        "train_mse = np.mean(np.power(X_train_scaled - train_reconstructions, 2), axis=1)\n",
        "\n",
        "# The anomaly threshold is established at the 93rd percentile of the TRAINING set\n",
        "cutoff_percentile = 93\n",
        "threshold = np.percentile(train_mse, cutoff_percentile)\n",
        "\n",
        "print(f\"\\nCalculated Threshold (Derived from Training Data): {threshold:.4f}\")\n",
        "\n",
        "# Apply Threshold to Test Data\n",
        "test_reconstructions = best_vae.predict(X_test_scaled)\n",
        "test_mse = np.mean(np.power(X_test_scaled - test_reconstructions, 2), axis=1)\n",
        "\n",
        "# Classification Report\n",
        "# Predictions are generated: 1 if Error > Threshold, else 0\n",
        "y_pred = (test_mse > threshold).astype(int)\n",
        "# --- MODIFIED SECTION END ---\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6BuJpjvYFgeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4 Analysis: Variational Autoencoder (VAE)\n",
        "\n",
        "**Ref:** Logic adapted from [Keras VAE Example](https://keras.io/examples/generative/vae/)\n",
        "\n",
        "\n",
        "#### 1. Training Strategy: “Best of 5”\n",
        "\n",
        "Variational Autoencoders are sensitive to random weight initialisation and can give noticeably different results from run to run. To reduce the impact of a single “unlucky” run, I trained the VAE **five** times from scratch.  For each run I tracked the **validation loss**, and the run with the **lowest validation loss** was kept as the final model.\n",
        "\n",
        "\n",
        "#### 2. Behaviour of the Runs\n",
        "\n",
        "Across the five runs, the validation loss and the decomposition into reconstruction loss and KL loss varied.  Some runs converged to higher validation loss, while others reached clearly better values, which supports using a “Best of 5” strategy rather than relying on a single training run. This shows that the architecture itself is workable, but its performance is sensitive to initialisation and optimisation noise.\n",
        "\n",
        "\n",
        "#### 3. Final Model Performance\n",
        "\n",
        "Using the best run (lowest validation loss) and a reconstruction‑error threshold at the **93rd percentile**, the VAE achieved on the held‑out test set:\n",
        "\n",
        "- **Recall (anomaly class)**: 0.57 / 57%\n",
        "- **Precision (anomaly class)**: 0.51 / 51%\n",
        "- **Accuracy (overall)**: 0.93 / 93%\n",
        "\n",
        "The model correctly flags a majority of anomalies but still misses some, and it also produces some false positives due to overlapping reconstruction errors between normal and anomalous cases.\n",
        "\n",
        "From the confusion matrix:\n",
        "- **True Negatives (TN)**: 8,510 normal transactions were correctly classified as normal.  \n",
        "- **False Positives (FP)**: 608 normal transactions were incorrectly flagged as anomalies.  \n",
        "- **False Negatives (FN)**: 662 anomalies were missed and classified as normal.  \n",
        "- **True Positives (TP)**: 80 anomalies were correctly identified as anomalous.\n",
        "\n",
        "#### 4. Latent Space Structure\n",
        "\n",
        "The encoder compresses each transaction into a 2‑dimensional latent vector.  When plotting these points, normal samples and anomalies form **distinct regions** with a visible diagonal separation, but there is also a **mixed band** where the two classes overlap.  This overlapping area explains why the model cannot achieve perfect precision or recall: some transactions sit in an ambiguous zone of the latent space.\n",
        "\n",
        "\n",
        "#### 5. VAE vs Standard Autoencoder\n",
        "\n",
        "Compared with the improved standard Autoencoder from Task 3 (anomaly recall ≈ **0.84**), the VAE’s best recall (≈ **0.57**) is lower.  The standard Autoencoder behaves more like a **discriminative** model, drawing sharp boundaries to catch most anomalies.  The VAE focuses on learning a smooth, continuous latent space, which is better for understanding and sampling from the data distribution, but tends to be more conservative for anomaly detection and therefore misses more subtle outliers.\n"
      ],
      "metadata": {
        "id": "xtJ6mOBWuqol"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}